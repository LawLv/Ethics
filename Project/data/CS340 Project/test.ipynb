{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:53.631124Z",
     "iopub.status.busy": "2024-04-24T02:00:53.630786Z",
     "iopub.status.idle": "2024-04-24T02:00:55.689613Z",
     "shell.execute_reply": "2024-04-24T02:00:55.688661Z",
     "shell.execute_reply.started": "2024-04-24T02:00:53.631056Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:55.693553Z",
     "iopub.status.busy": "2024-04-24T02:00:55.693292Z",
     "iopub.status.idle": "2024-04-24T02:01:23.715538Z",
     "shell.execute_reply": "2024-04-24T02:01:23.714810Z",
     "shell.execute_reply.started": "2024-04-24T02:00:55.693499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1804874 records\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./Data/train.csv')\n",
    "print('loaded %d records' % len(train))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str) \n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "train = convert_dataframe_to_bool(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into 80% train and 20% validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:23.717481Z",
     "iopub.status.busy": "2024-04-24T02:01:23.717163Z",
     "iopub.status.idle": "2024-04-24T02:01:25.493770Z",
     "shell.execute_reply": "2024-04-24T02:01:25.492828Z",
     "shell.execute_reply.started": "2024-04-24T02:01:23.717421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443899 train comments, 360975 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:25.495531Z",
     "iopub.status.busy": "2024-04-24T02:01:25.495199Z",
     "iopub.status.idle": "2024-04-24T02:03:01.848534Z",
     "shell.execute_reply": "2024-04-24T02:03:01.847824Z",
     "shell.execute_reply.started": "2024-04-24T02:01:25.495454Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a Convolutional Neural Net for classifying toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 这里使用保存好的\n",
    "with open('my_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11281/11281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "validate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>my_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168790</th>\n",
       "      <td>448414</td>\n",
       "      <td>False</td>\n",
       "      <td>They're atmospheric scientists. There's nothin...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.535743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053007</th>\n",
       "      <td>5404258</td>\n",
       "      <td>False</td>\n",
       "      <td>Did you miss the over one year of discussions,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746044</th>\n",
       "      <td>6262511</td>\n",
       "      <td>False</td>\n",
       "      <td>First, a sitting president can't be indicted\\n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150097</th>\n",
       "      <td>425826</td>\n",
       "      <td>False</td>\n",
       "      <td>I'm far from the only one. There are millions ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.007217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793908</th>\n",
       "      <td>6319838</td>\n",
       "      <td>False</td>\n",
       "      <td>LOL!  No, I'm proud of my Ducks.  Their goal l...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780006</th>\n",
       "      <td>6303364</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes, we do.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987136</th>\n",
       "      <td>5325275</td>\n",
       "      <td>False</td>\n",
       "      <td>glad you also noticed that....\\nalso there is ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748910</th>\n",
       "      <td>6265972</td>\n",
       "      <td>False</td>\n",
       "      <td>True, but Dallas also tried to trade up to the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290610</th>\n",
       "      <td>5691964</td>\n",
       "      <td>False</td>\n",
       "      <td>Please stop misrepresenting DeTocqueville. To ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513367</th>\n",
       "      <td>872141</td>\n",
       "      <td>False</td>\n",
       "      <td>You are conveniently ignoring the fact that th...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.336717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567181</th>\n",
       "      <td>6039311</td>\n",
       "      <td>False</td>\n",
       "      <td>This is exactly how I feel.  I have Medicare n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302116</th>\n",
       "      <td>612079</td>\n",
       "      <td>False</td>\n",
       "      <td>The climate has been changing for over a 1000 ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.024953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634407</th>\n",
       "      <td>1017874</td>\n",
       "      <td>False</td>\n",
       "      <td>I had not heard the anecdote about the busines...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686663</th>\n",
       "      <td>6189729</td>\n",
       "      <td>False</td>\n",
       "      <td>I am Catholic.  I believe abortion is wrong.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694795</th>\n",
       "      <td>4968918</td>\n",
       "      <td>False</td>\n",
       "      <td>You don't \"wait around to charge\". You charge ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525864</th>\n",
       "      <td>5988719</td>\n",
       "      <td>False</td>\n",
       "      <td>We don't live on geologic scales,  so yes,  re...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713929</th>\n",
       "      <td>4996066</td>\n",
       "      <td>False</td>\n",
       "      <td>...but big money wins the day because campaign...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587824</th>\n",
       "      <td>6065006</td>\n",
       "      <td>True</td>\n",
       "      <td>What is wrong with these people? They are as b...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651371</th>\n",
       "      <td>1039625</td>\n",
       "      <td>False</td>\n",
       "      <td>I just watched Trump's  speech  , and if you f...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558841</th>\n",
       "      <td>926619</td>\n",
       "      <td>False</td>\n",
       "      <td>Even the Pope's authority is limited, and the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "168790    448414   False  They're atmospheric scientists. There's nothin...   \n",
       "1053007  5404258   False  Did you miss the over one year of discussions,...   \n",
       "1746044  6262511   False  First, a sitting president can't be indicted\\n...   \n",
       "150097    425826   False  I'm far from the only one. There are millions ...   \n",
       "1793908  6319838   False  LOL!  No, I'm proud of my Ducks.  Their goal l...   \n",
       "1780006  6303364   False                                        Yes, we do.   \n",
       "987136   5325275   False  glad you also noticed that....\\nalso there is ...   \n",
       "1748910  6265972   False  True, but Dallas also tried to trade up to the...   \n",
       "1290610  5691964   False  Please stop misrepresenting DeTocqueville. To ...   \n",
       "513367    872141   False  You are conveniently ignoring the fact that th...   \n",
       "1567181  6039311   False  This is exactly how I feel.  I have Medicare n...   \n",
       "302116    612079   False  The climate has been changing for over a 1000 ...   \n",
       "634407   1017874   False  I had not heard the anecdote about the busines...   \n",
       "1686663  6189729   False       I am Catholic.  I believe abortion is wrong.   \n",
       "694795   4968918   False  You don't \"wait around to charge\". You charge ...   \n",
       "1525864  5988719   False  We don't live on geologic scales,  so yes,  re...   \n",
       "713929   4996066   False  ...but big money wins the day because campaign...   \n",
       "1587824  6065006    True  What is wrong with these people? They are as b...   \n",
       "651371   1039625   False  I just watched Trump's  speech  , and if you f...   \n",
       "558841    926619   False  Even the Pope's authority is limited, and the ...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult    threat  \\\n",
       "168790          0.000000  0.000000         0.000000  0.166667  0.000000   \n",
       "1053007         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1746044         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "150097          0.000000  0.000000         0.300000  0.400000  0.000000   \n",
       "1793908         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1780006         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "987136          0.000000  0.000000         0.166667  0.166667  0.166667   \n",
       "1748910         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1290610         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "513367          0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1567181         0.000000  0.000000         0.000000  0.200000  0.000000   \n",
       "302116          0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "634407          0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1686663         0.000000  0.000000         0.000000  0.200000  0.100000   \n",
       "694795          0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "1525864         0.000000  0.000000         0.000000  0.000000  0.000000   \n",
       "713929          0.000000  0.000000         0.166667  0.166667  0.000000   \n",
       "1587824         0.046875  0.078125         0.062500  0.671875  0.046875   \n",
       "651371          0.000000  0.000000         0.000000  0.200000  0.000000   \n",
       "558841          0.000000  0.000000         0.000000  0.000000  0.166667   \n",
       "\n",
       "         asian  atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "168790     NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "1053007    NaN      NaN  ...  approved      1    0    0      3         0   \n",
       "1746044    NaN      NaN  ...  approved      0    0    0      0         1   \n",
       "150097     0.0      0.0  ...  approved      0    0    0      0         0   \n",
       "1793908    NaN      NaN  ...  approved      3    0    0      0         0   \n",
       "1780006    0.0      0.0  ...  approved      0    0    0      0         0   \n",
       "987136     NaN      NaN  ...  approved      0    0    0      2         0   \n",
       "1748910    NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "1290610    NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "513367     NaN      NaN  ...  approved      0    0    0      2         0   \n",
       "1567181    NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "302116     NaN      NaN  ...  approved      0    0    0     26         0   \n",
       "634407     NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "1686663    0.0      0.0  ...  approved      0    0    0      2         0   \n",
       "694795     NaN      NaN  ...  approved      0    0    0      2         2   \n",
       "1525864    NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "713929     NaN      NaN  ...  approved      1    1    0      3         0   \n",
       "1587824    NaN      NaN  ...  rejected      0    0    0      1         2   \n",
       "651371     0.0      0.0  ...  approved      0    0    0      7         2   \n",
       "558841     NaN      NaN  ...  approved      0    0    0      0         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n",
       "168790               0.0                         0                         6   \n",
       "1053007              0.0                         0                         4   \n",
       "1746044              0.0                         0                         4   \n",
       "150097               0.0                        10                        10   \n",
       "1793908              0.0                         0                         4   \n",
       "1780006              0.0                         4                         5   \n",
       "987136               0.0                         0                         6   \n",
       "1748910              0.0                         0                         4   \n",
       "1290610              0.0                         0                         4   \n",
       "513367               0.0                         0                         4   \n",
       "1567181              0.0                         0                        10   \n",
       "302116               0.0                         0                         6   \n",
       "634407               0.0                         0                         4   \n",
       "1686663              0.0                         6                        10   \n",
       "694795               0.0                         0                         4   \n",
       "1525864              0.0                         0                         4   \n",
       "713929               0.0                         0                         6   \n",
       "1587824              0.0                         0                        64   \n",
       "651371               0.0                         5                         5   \n",
       "558841               0.0                         0                         6   \n",
       "\n",
       "         my_model  \n",
       "168790   0.535743  \n",
       "1053007  0.002325  \n",
       "1746044  0.006512  \n",
       "150097   0.007217  \n",
       "1793908  0.005643  \n",
       "1780006  0.002806  \n",
       "987136   0.006407  \n",
       "1748910  0.032981  \n",
       "1290610  0.025400  \n",
       "513367   0.336717  \n",
       "1567181  0.026747  \n",
       "302116   0.024953  \n",
       "634407   0.005239  \n",
       "1686663  0.028774  \n",
       "694795   0.001832  \n",
       "1525864  0.010096  \n",
       "713929   0.001618  \n",
       "1587824  0.006640  \n",
       "651371   0.021906  \n",
       "558841   0.014932  \n",
       "\n",
       "[20 rows x 46 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define bias metrics, then evaluate our new model for bias using the validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_12968\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>3031</td>\n",
       "      <td>0.532548</td>\n",
       "      <td>0.452617</td>\n",
       "      <td>0.658422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>972</td>\n",
       "      <td>0.549919</td>\n",
       "      <td>0.269056</td>\n",
       "      <td>0.815847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>8211</td>\n",
       "      <td>0.557027</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.599770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>4157</td>\n",
       "      <td>0.561796</td>\n",
       "      <td>0.571698</td>\n",
       "      <td>0.575926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>1533</td>\n",
       "      <td>0.571313</td>\n",
       "      <td>0.597413</td>\n",
       "      <td>0.555505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>5200</td>\n",
       "      <td>0.574280</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>0.792648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>2184</td>\n",
       "      <td>0.577169</td>\n",
       "      <td>0.530998</td>\n",
       "      <td>0.631610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>10598</td>\n",
       "      <td>0.589578</td>\n",
       "      <td>0.501455</td>\n",
       "      <td>0.669298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>9011</td>\n",
       "      <td>0.609013</td>\n",
       "      <td>0.474290</td>\n",
       "      <td>0.709780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "6                          black           3031      0.532548  0.452617   \n",
       "8  psychiatric_or_mental_illness            972      0.549919  0.269056   \n",
       "3                      christian           8211      0.557027  0.542359   \n",
       "5                         muslim           4157      0.561796  0.571698   \n",
       "4                         jewish           1533      0.571313  0.597413   \n",
       "7                          white           5200      0.574280  0.332900   \n",
       "2      homosexual_gay_or_lesbian           2184      0.577169  0.530998   \n",
       "1                         female          10598      0.589578  0.501455   \n",
       "0                           male           9011      0.609013  0.474290   \n",
       "\n",
       "   bnsp_auc  \n",
       "6  0.658422  \n",
       "8  0.815847  \n",
       "3  0.599770  \n",
       "5  0.575926  \n",
       "4  0.555505  \n",
       "7  0.792648  \n",
       "2  0.631610  \n",
       "1  0.669298  \n",
       "0  0.709780  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88432813949746"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./Data/test.csv')\n",
    "submission = pd.read_csv('./Data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3042/3042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\3024343552.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comment  \\\n",
      "0  Dave, I agree the was one of the most entertaining baseball games ever.  I often find baseball boring, especially if I don't have a friend or relative playing.  Game 2 was a great battle going back and forth.  How about an umpire taking a ball to his crotch when a steal at second base was in play!                                                                                                                                                                                                                                                  \n",
      "1  God save the Queen of Canada                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "2  Sounds like a demonic possession or perhaps alien related to me. Whatever, an \"odd,squid-pulsing sensation\" on the brain should always be a cause of concern..                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "3  Hmm ... over 100 \"illegal\" dispensaries popped-up in Toronto alone last year, and the Liberal plan is to have 40 outlets up and running across the entire province by July next year.  \\n\\nAnd they expect to \"choke-off\" the black market with this kind of supply-demand equation?                                                                                                                                                                                                                                                                        \n",
      "4  Thank you for the clarification. What makes the situation so dire now is that it is not just government largess that is offered by elected officials but their generosity (at the federal level) uses vast sums of borrowed money.  Also, at all levels,  elected officials now engage in a sustained effort to choose winners and losers among those who would otherwise be expected to function in a free market economy using innate ability. The classic example of this in Alaska is the Alaska Industrial Development and Export Authority (AIDEA).   \n",
      "5  In answer to the headline - who cares.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "6  And, IRT PostManx, it's still under ACA or Obamacare.  If the U.S. Senate drag their feet in presenting a healthcare plan, the current costs are going higher because Hawaii born ex-President Obama kicked the Obamacare can into 2017.  Auwe.                                                                                                                                                                                                                                                                                                             \n",
      "7  I'm sure the transgender kid was a plant by Liberal strategists. Pretty pathetic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "8  I don't do polls - you are citing the same pollsters who predicted a Hillary win. Obama had no risk associated with sending a seal team to murder Bin Laden. And Obamacare increase the cost for just about everybody, while deadbeats got it for free, and before you say what about the children, every state I have lived in insures children for free.                                                                                                                                                                                                  \n",
      "9  Well, a few years ago, he was equipped with work boots a hard hat, and air hammer over the community mailbox dustup...so maybe tomorrow he'll be reporting for work somewhere in Quebec.                                                                                                                                                                                                                                                                                                                                                                    \n",
      "\n",
      "   predicted_label  prediction_score  \n",
      "0  0                0.003540          \n",
      "1  0                0.016491          \n",
      "2  0                0.090685          \n",
      "3  0                0.033048          \n",
      "4  0                0.000952          \n",
      "5  0                0.010418          \n",
      "6  0                0.002903          \n",
      "7  1                0.973218          \n",
      "8  0                0.002276          \n",
      "9  0                0.009896          \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# 假设我们已经有一个训练好的模型和验证数据集 validate_df\n",
    "# 并且模型已经预测了验证集的结果\n",
    "MODEL_NAME = 'my_model'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TOXICITY_COLUMN = 'toxic'\n",
    "\n",
    "# 随机选择10条评论\n",
    "num_samples = 10\n",
    "sampled_comments = validate_df.sample(num_samples, random_state=5)\n",
    "\n",
    "\n",
    "# 获取模型对这些评论的预测结果\n",
    "predictions = model.predict(pad_text(sampled_comments[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# 创建一个数据框来比较结果\n",
    "comparison_df = pd.DataFrame({\n",
    "    'comment': sampled_comments[TEXT_COLUMN].values,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'prediction_score': predictions\n",
    "})\n",
    "\n",
    "# 打印比较结果\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validate_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 计算每个子群体的正类预测比例\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m sensitive_attributes:\n\u001b[1;32m---> 11\u001b[0m     subgroup_data \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_df\u001b[49m[validate_df[attribute] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     12\u001b[0m     positive_rate \u001b[38;5;241m=\u001b[39m subgroup_data[MODEL_NAME]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     13\u001b[0m     subgroup_positive_rate[attribute] \u001b[38;5;241m=\u001b[39m positive_rate\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validate_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义敏感属性\n",
    "sensitive_attributes = ['male', 'female', 'christian', 'muslim', 'jewish', 'white', 'black']\n",
    "\n",
    "# 初始化一个字典来存储各子群体的比例\n",
    "subgroup_positive_rate = {}\n",
    "\n",
    "# 计算每个子群体的正类预测比例\n",
    "for attribute in sensitive_attributes:\n",
    "    subgroup_data = validate_df[validate_df[attribute] == 1]\n",
    "    positive_rate = subgroup_data[MODEL_NAME].mean()\n",
    "    subgroup_positive_rate[attribute] = positive_rate\n",
    "\n",
    "# 计算总体的正类预测比例\n",
    "overall_positive_rate = validate_df[MODEL_NAME].mean()\n",
    "\n",
    "# 打印结果\n",
    "print(\"Overall Positive Rate: {:.4f}\".format(overall_positive_rate))\n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    print(\"Subgroup '{}' Positive Rate: {:.4f}\".format(attribute, rate))\n",
    "\n",
    "# 比较各子群体的预测比例与总体预测比例\n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    disparity = abs(rate - overall_positive_rate)\n",
    "    print(\"Disparity difference for '{}': {:.4f}\".format(attribute, disparity))\n",
    "    \n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    disparity = abs(overall_positive_rate / rate)\n",
    "    print(\"Disparity ratio for '{}': {:.4f}\".format(attribute, disparity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "import pickle\n",
    "\n",
    "# save the model\n",
    "with open('my_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 23026,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
