{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:53.631124Z",
     "iopub.status.busy": "2024-04-24T02:00:53.630786Z",
     "iopub.status.idle": "2024-04-24T02:00:55.689613Z",
     "shell.execute_reply": "2024-04-24T02:00:55.688661Z",
     "shell.execute_reply.started": "2024-04-24T02:00:53.631056Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:00:55.693553Z",
     "iopub.status.busy": "2024-04-24T02:00:55.693292Z",
     "iopub.status.idle": "2024-04-24T02:01:23.715538Z",
     "shell.execute_reply": "2024-04-24T02:01:23.714810Z",
     "shell.execute_reply.started": "2024-04-24T02:00:55.693499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1804874 records\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./Data/train.csv')\n",
    "print('loaded %d records' % len(train))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str) \n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "train = convert_dataframe_to_bool(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into 80% train and 20% validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:23.717481Z",
     "iopub.status.busy": "2024-04-24T02:01:23.717163Z",
     "iopub.status.idle": "2024-04-24T02:01:25.493770Z",
     "shell.execute_reply": "2024-04-24T02:01:25.492828Z",
     "shell.execute_reply.started": "2024-04-24T02:01:23.717421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443899 train comments, 360975 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:01:25.495531Z",
     "iopub.status.busy": "2024-04-24T02:01:25.495199Z",
     "iopub.status.idle": "2024-04-24T02:03:01.848534Z",
     "shell.execute_reply": "2024-04-24T02:03:01.847824Z",
     "shell.execute_reply.started": "2024-04-24T02:01:25.495454Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a Convolutional Neural Net for classifying toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:03:01.850248Z",
     "iopub.status.busy": "2024-04-24T02:03:01.850018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings\n",
      "compiling model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "Epoch 1/10\n",
      "11281/11281 - 1059s - 94ms/step - acc: 0.9326 - loss: 0.1966 - val_acc: 0.9376 - val_loss: 0.1724\n",
      "Epoch 2/10\n",
      "11281/11281 - 969s - 86ms/step - acc: 0.9422 - loss: 0.1626 - val_acc: 0.9437 - val_loss: 0.1576\n",
      "Epoch 3/10\n",
      "11281/11281 - 921s - 82ms/step - acc: 0.9444 - loss: 0.1548 - val_acc: 0.9450 - val_loss: 0.1536\n",
      "Epoch 4/10\n",
      "11281/11281 - 908s - 80ms/step - acc: 0.9456 - loss: 0.1504 - val_acc: 0.9455 - val_loss: 0.1511\n",
      "Epoch 5/10\n",
      "11281/11281 - 850s - 75ms/step - acc: 0.9463 - loss: 0.1476 - val_acc: 0.9456 - val_loss: 0.1517\n",
      "Epoch 6/10\n",
      "11281/11281 - 1042s - 92ms/step - acc: 0.9470 - loss: 0.1455 - val_acc: 0.9451 - val_loss: 0.1512\n",
      "Epoch 7/10\n",
      "11281/11281 - 1028s - 91ms/step - acc: 0.9474 - loss: 0.1439 - val_acc: 0.9460 - val_loss: 0.1497\n",
      "Epoch 8/10\n",
      "11281/11281 - 969s - 86ms/step - acc: 0.9477 - loss: 0.1427 - val_acc: 0.9450 - val_loss: 0.1505\n",
      "Epoch 9/10\n",
      "11281/11281 - 795s - 71ms/step - acc: 0.9481 - loss: 0.1414 - val_acc: 0.9455 - val_loss: 0.1494\n",
      "Epoch 10/10\n",
      "11281/11281 - 837s - 74ms/step - acc: 0.9483 - loss: 0.1406 - val_acc: 0.9459 - val_loss: 0.1490\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_PATH = './Embedding_file/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n",
    "\n",
    "    # Load embeddings\n",
    "    print('loading embeddings')\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                    EMBEDDINGS_DIMENSION,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    print('compiling model')\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    print('training model')\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(train_df, validate_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11281/11281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'my_model'\n",
    "validate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>my_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1601749</th>\n",
       "      <td>6081961</td>\n",
       "      <td>False</td>\n",
       "      <td>I haven't read the article yet, and am incline...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252708</th>\n",
       "      <td>5646267</td>\n",
       "      <td>True</td>\n",
       "      <td>A man dies and all you want to do is blame it ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.938627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516364</th>\n",
       "      <td>875620</td>\n",
       "      <td>False</td>\n",
       "      <td>Koncerned,\\n\\n     We should be looking at sal...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35165</th>\n",
       "      <td>284866</td>\n",
       "      <td>False</td>\n",
       "      <td>For every nominee that the Republican majority...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172552</th>\n",
       "      <td>5548794</td>\n",
       "      <td>False</td>\n",
       "      <td>The only reason why he's a Canadian is that hi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492860</th>\n",
       "      <td>848141</td>\n",
       "      <td>False</td>\n",
       "      <td>(Cont'd)\\nThat leadership must a) demonstrate ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291512</th>\n",
       "      <td>598895</td>\n",
       "      <td>False</td>\n",
       "      <td>Both Duffy and Wright should be prohibited fro...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.003448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422427</th>\n",
       "      <td>760662</td>\n",
       "      <td>False</td>\n",
       "      <td>no it's not Y+A=    No it's not zero populatio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.228106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609610</th>\n",
       "      <td>6092261</td>\n",
       "      <td>False</td>\n",
       "      <td>Instead of copying the company news release, i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>278708</td>\n",
       "      <td>False</td>\n",
       "      <td>LM - You have no state taxes withheld from you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148518</th>\n",
       "      <td>5520794</td>\n",
       "      <td>False</td>\n",
       "      <td>We just like yelling at the them on the televi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.058472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423537</th>\n",
       "      <td>5858627</td>\n",
       "      <td>False</td>\n",
       "      <td>The people elected Arpaio and Trump, notwithst...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701183</th>\n",
       "      <td>4980643</td>\n",
       "      <td>False</td>\n",
       "      <td>At least Protestants are open about their divi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27069</th>\n",
       "      <td>275016</td>\n",
       "      <td>False</td>\n",
       "      <td>You mean the same bloated budgets the liberal ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42805</th>\n",
       "      <td>293999</td>\n",
       "      <td>False</td>\n",
       "      <td>There is more to this giveaway than most Alask...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910176</th>\n",
       "      <td>5233402</td>\n",
       "      <td>False</td>\n",
       "      <td>That's not what I said at all actually...what ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099240</th>\n",
       "      <td>5459783</td>\n",
       "      <td>False</td>\n",
       "      <td>I think they're running concurrently.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641516</th>\n",
       "      <td>1027419</td>\n",
       "      <td>False</td>\n",
       "      <td>Sheep, this article has nothing to do with G a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489097</th>\n",
       "      <td>5942531</td>\n",
       "      <td>False</td>\n",
       "      <td>Agree, however, not sure it's possible at the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105156</th>\n",
       "      <td>370769</td>\n",
       "      <td>False</td>\n",
       "      <td>Where do you recommend going to u-pick? I kept...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "1601749  6081961   False  I haven't read the article yet, and am incline...   \n",
       "1252708  5646267    True  A man dies and all you want to do is blame it ...   \n",
       "516364    875620   False  Koncerned,\\n\\n     We should be looking at sal...   \n",
       "35165     284866   False  For every nominee that the Republican majority...   \n",
       "1172552  5548794   False  The only reason why he's a Canadian is that hi...   \n",
       "492860    848141   False  (Cont'd)\\nThat leadership must a) demonstrate ...   \n",
       "291512    598895   False  Both Duffy and Wright should be prohibited fro...   \n",
       "422427    760662   False  no it's not Y+A=    No it's not zero populatio...   \n",
       "1609610  6092261   False  Instead of copying the company news release, i...   \n",
       "30107     278708   False  LM - You have no state taxes withheld from you...   \n",
       "1148518  5520794   False  We just like yelling at the them on the televi...   \n",
       "1423537  5858627   False  The people elected Arpaio and Trump, notwithst...   \n",
       "701183   4980643   False  At least Protestants are open about their divi...   \n",
       "27069     275016   False  You mean the same bloated budgets the liberal ...   \n",
       "42805     293999   False  There is more to this giveaway than most Alask...   \n",
       "910176   5233402   False  That's not what I said at all actually...what ...   \n",
       "1099240  5459783   False              I think they're running concurrently.   \n",
       "641516   1027419   False  Sheep, this article has nothing to do with G a...   \n",
       "1489097  5942531   False  Agree, however, not sure it's possible at the ...   \n",
       "105156    370769   False  Where do you recommend going to u-pick? I kept...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "1601749              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1252708              0.0  0.166667              0.0  0.833333     0.0    0.0   \n",
       "516364               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "35165                0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1172552              0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "492860               0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "291512               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "422427               0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "1609610              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "30107                0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1148518              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1423537              0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "701183               0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "27069                0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "42805                0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "910176               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1099240              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "641516               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1489097              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "105156               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "\n",
       "         atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "1601749      NaN  ...  approved      0    0    0      2         0   \n",
       "1252708      0.0  ...  approved      1    0    1      9         0   \n",
       "516364       NaN  ...  approved      0    0    0      2         2   \n",
       "35165        NaN  ...  approved      0    0    0     10         0   \n",
       "1172552      0.0  ...  rejected      0    0    0      1         0   \n",
       "492860       0.0  ...  approved      0    0    0      5         0   \n",
       "291512       NaN  ...  approved      0    0    0      1         0   \n",
       "422427       0.0  ...  approved      0    1    0      0         0   \n",
       "1609610      NaN  ...  approved      0    0    0      0         0   \n",
       "30107        NaN  ...  approved      0    0    0      3         0   \n",
       "1148518      NaN  ...  approved      0    0    0      0         0   \n",
       "1423537      0.0  ...  approved      0    0    2      5        11   \n",
       "701183       0.0  ...  approved      0    0    0      3         0   \n",
       "27069        NaN  ...  approved      0    0    0      1         0   \n",
       "42805        0.0  ...  approved      0    0    0      6         0   \n",
       "910176       NaN  ...  approved      0    0    0      0         0   \n",
       "1099240      NaN  ...  approved      0    1    0      0         0   \n",
       "641516       NaN  ...  rejected      0    0    0      0         0   \n",
       "1489097      NaN  ...  approved      0    0    0      1         1   \n",
       "105156       NaN  ...  approved      0    0    0      0         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n",
       "1601749              0.0                         0                         4   \n",
       "1252708              0.0                         4                         6   \n",
       "516364               0.0                         0                         4   \n",
       "35165                0.0                         0                         4   \n",
       "1172552              0.0                         4                         4   \n",
       "492860               0.0                        10                         4   \n",
       "291512               0.0                         0                         6   \n",
       "422427               0.0                         4                         4   \n",
       "1609610              0.0                         0                         4   \n",
       "30107                0.0                         0                         4   \n",
       "1148518              0.0                         0                         4   \n",
       "1423537              0.0                         4                         4   \n",
       "701183               0.0                        10                         4   \n",
       "27069                0.0                         0                         4   \n",
       "42805                0.0                         5                         4   \n",
       "910176               0.0                         0                         4   \n",
       "1099240              0.0                         0                         4   \n",
       "641516               0.0                         0                         4   \n",
       "1489097              0.0                         0                         4   \n",
       "105156               0.0                         0                         4   \n",
       "\n",
       "         my_model  \n",
       "1601749  0.001639  \n",
       "1252708  0.938627  \n",
       "516364   0.002605  \n",
       "35165    0.003578  \n",
       "1172552  0.004690  \n",
       "492860   0.019165  \n",
       "291512   0.003448  \n",
       "422427   0.228106  \n",
       "1609610  0.000977  \n",
       "30107    0.005290  \n",
       "1148518  0.058472  \n",
       "1423537  0.017417  \n",
       "701183   0.036190  \n",
       "27069    0.008876  \n",
       "42805    0.000534  \n",
       "910176   0.001434  \n",
       "1099240  0.004349  \n",
       "641516   0.002043  \n",
       "1489097  0.001473  \n",
       "105156   0.003119  \n",
       "\n",
       "[20 rows x 46 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define bias metrics, then evaluate our new model for bias using the validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\594028158.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>2171</td>\n",
       "      <td>0.796211</td>\n",
       "      <td>0.773837</td>\n",
       "      <td>0.953297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>2950</td>\n",
       "      <td>0.799785</td>\n",
       "      <td>0.767659</td>\n",
       "      <td>0.956518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>4246</td>\n",
       "      <td>0.819911</td>\n",
       "      <td>0.816852</td>\n",
       "      <td>0.945829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>4937</td>\n",
       "      <td>0.821602</td>\n",
       "      <td>0.768996</td>\n",
       "      <td>0.961558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>1469</td>\n",
       "      <td>0.841897</td>\n",
       "      <td>0.863368</td>\n",
       "      <td>0.927950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>995</td>\n",
       "      <td>0.872422</td>\n",
       "      <td>0.858607</td>\n",
       "      <td>0.945082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>8875</td>\n",
       "      <td>0.875165</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>0.943594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>10670</td>\n",
       "      <td>0.880328</td>\n",
       "      <td>0.878394</td>\n",
       "      <td>0.936239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>8092</td>\n",
       "      <td>0.893468</td>\n",
       "      <td>0.913464</td>\n",
       "      <td>0.916502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           2171      0.796211  0.773837   \n",
       "6                          black           2950      0.799785  0.767659   \n",
       "5                         muslim           4246      0.819911  0.816852   \n",
       "7                          white           4937      0.821602  0.768996   \n",
       "4                         jewish           1469      0.841897  0.863368   \n",
       "8  psychiatric_or_mental_illness            995      0.872422  0.858607   \n",
       "0                           male           8875      0.875165  0.862522   \n",
       "1                         female          10670      0.880328  0.878394   \n",
       "3                      christian           8092      0.893468  0.913464   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.953297  \n",
       "6  0.956518  \n",
       "5  0.945829  \n",
       "7  0.961558  \n",
       "4  0.927950  \n",
       "8  0.945082  \n",
       "0  0.943594  \n",
       "1  0.936239  \n",
       "3  0.916502  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88432813949746"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./Data/test.csv')\n",
    "submission = pd.read_csv('./Data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3042/3042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Parker\\AppData\\Local\\Temp\\ipykernel_20260\\3024343552.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comment  \\\n",
      "0  Dave, I agree the was one of the most entertaining baseball games ever.  I often find baseball boring, especially if I don't have a friend or relative playing.  Game 2 was a great battle going back and forth.  How about an umpire taking a ball to his crotch when a steal at second base was in play!                                                                                                                                                                                                                                                  \n",
      "1  God save the Queen of Canada                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "2  Sounds like a demonic possession or perhaps alien related to me. Whatever, an \"odd,squid-pulsing sensation\" on the brain should always be a cause of concern..                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "3  Hmm ... over 100 \"illegal\" dispensaries popped-up in Toronto alone last year, and the Liberal plan is to have 40 outlets up and running across the entire province by July next year.  \\n\\nAnd they expect to \"choke-off\" the black market with this kind of supply-demand equation?                                                                                                                                                                                                                                                                        \n",
      "4  Thank you for the clarification. What makes the situation so dire now is that it is not just government largess that is offered by elected officials but their generosity (at the federal level) uses vast sums of borrowed money.  Also, at all levels,  elected officials now engage in a sustained effort to choose winners and losers among those who would otherwise be expected to function in a free market economy using innate ability. The classic example of this in Alaska is the Alaska Industrial Development and Export Authority (AIDEA).   \n",
      "5  In answer to the headline - who cares.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "6  And, IRT PostManx, it's still under ACA or Obamacare.  If the U.S. Senate drag their feet in presenting a healthcare plan, the current costs are going higher because Hawaii born ex-President Obama kicked the Obamacare can into 2017.  Auwe.                                                                                                                                                                                                                                                                                                             \n",
      "7  I'm sure the transgender kid was a plant by Liberal strategists. Pretty pathetic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "8  I don't do polls - you are citing the same pollsters who predicted a Hillary win. Obama had no risk associated with sending a seal team to murder Bin Laden. And Obamacare increase the cost for just about everybody, while deadbeats got it for free, and before you say what about the children, every state I have lived in insures children for free.                                                                                                                                                                                                  \n",
      "9  Well, a few years ago, he was equipped with work boots a hard hat, and air hammer over the community mailbox dustup...so maybe tomorrow he'll be reporting for work somewhere in Quebec.                                                                                                                                                                                                                                                                                                                                                                    \n",
      "\n",
      "   predicted_label  prediction_score  \n",
      "0  0                0.003540          \n",
      "1  0                0.016491          \n",
      "2  0                0.090685          \n",
      "3  0                0.033048          \n",
      "4  0                0.000952          \n",
      "5  0                0.010418          \n",
      "6  0                0.002903          \n",
      "7  1                0.973218          \n",
      "8  0                0.002276          \n",
      "9  0                0.009896          \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# 假设我们已经有一个训练好的模型和验证数据集 validate_df\n",
    "# 并且模型已经预测了验证集的结果\n",
    "MODEL_NAME = 'my_model'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TOXICITY_COLUMN = 'toxic'\n",
    "\n",
    "# 随机选择10条评论\n",
    "num_samples = 10\n",
    "sampled_comments = validate_df.sample(num_samples, random_state=5)\n",
    "\n",
    "\n",
    "# 获取模型对这些评论的预测结果\n",
    "predictions = model.predict(pad_text(sampled_comments[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# 创建一个数据框来比较结果\n",
    "comparison_df = pd.DataFrame({\n",
    "    'comment': sampled_comments[TEXT_COLUMN].values,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'prediction_score': predictions\n",
    "})\n",
    "\n",
    "# 打印比较结果\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Positive Rate: 0.0690\n",
      "Subgroup 'male' Positive Rate: 0.1263\n",
      "Subgroup 'female' Positive Rate: 0.1110\n",
      "Subgroup 'christian' Positive Rate: 0.0704\n",
      "Subgroup 'muslim' Positive Rate: 0.1662\n",
      "Subgroup 'jewish' Positive Rate: 0.1175\n",
      "Subgroup 'white' Positive Rate: 0.2253\n",
      "Subgroup 'black' Positive Rate: 0.2327\n",
      "Disparity difference for 'male': 0.0573\n",
      "Disparity difference for 'female': 0.0420\n",
      "Disparity difference for 'christian': 0.0014\n",
      "Disparity difference for 'muslim': 0.0972\n",
      "Disparity difference for 'jewish': 0.0484\n",
      "Disparity difference for 'white': 0.1563\n",
      "Disparity difference for 'black': 0.1637\n",
      "Disparity ratio for 'male': 0.5466\n",
      "Disparity ratio for 'female': 0.6217\n",
      "Disparity ratio for 'christian': 0.9803\n",
      "Disparity ratio for 'muslim': 0.4154\n",
      "Disparity ratio for 'jewish': 0.5877\n",
      "Disparity ratio for 'white': 0.3064\n",
      "Disparity ratio for 'black': 0.2967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义敏感属性\n",
    "sensitive_attributes = ['male', 'female', 'christian', 'muslim', 'jewish', 'white', 'black']\n",
    "\n",
    "# 初始化一个字典来存储各子群体的比例\n",
    "subgroup_positive_rate = {}\n",
    "\n",
    "# 计算每个子群体的正类预测比例\n",
    "for attribute in sensitive_attributes:\n",
    "    subgroup_data = validate_df[validate_df[attribute] == 1]\n",
    "    positive_rate = subgroup_data[MODEL_NAME].mean()\n",
    "    subgroup_positive_rate[attribute] = positive_rate\n",
    "\n",
    "# 计算总体的正类预测比例\n",
    "overall_positive_rate = validate_df[MODEL_NAME].mean()\n",
    "\n",
    "# 打印结果\n",
    "print(\"Overall Positive Rate: {:.4f}\".format(overall_positive_rate))\n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    print(\"Subgroup '{}' Positive Rate: {:.4f}\".format(attribute, rate))\n",
    "\n",
    "# 比较各子群体的预测比例与总体预测比例\n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    disparity = abs(rate - overall_positive_rate)\n",
    "    print(\"Disparity difference for '{}': {:.4f}\".format(attribute, disparity))\n",
    "    \n",
    "for attribute, rate in subgroup_positive_rate.items():\n",
    "    disparity = abs(overall_positive_rate / rate)\n",
    "    print(\"Disparity ratio for '{}': {:.4f}\".format(attribute, disparity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 23026,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
